{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMWve/c7BT1uUuomX9KB8cu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1.&nbsp;Intro"],"metadata":{"id":"QH6CoiUyrUSo"}},{"cell_type":"code","source":["import numpy as np\n","import cv2 as cv\n","import matplotlib.pyplot as plt\n","\n","\n","image = cv.imread(\"Images/cityscape.jpg\", cv.IMREAD_GRAYSCALE)\n","\n","cv.imwrite(\"Images/cityscape2.jpg\", image)\n","#cv.IMREAD_GRAYSCALE    0\n","#cv.IMREAD_COLOR        1\n","#cv.IMREAD_UNCHANGED    -1\n","\n","\n","\"\"\"\n","cv.imshow('IMAGE display', image)\n","cv.waitKey(0)\n","cv.destroyAllWindows()\n","\"\"\"\n","\n","plt.imshow(image)\n","plt.imshow()"],"metadata":{"id":"0wt_r2srtwhG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.&nbsp;Drawingshapes"],"metadata":{"id":"CksqZ3q5rUQZ"}},{"cell_type":"code","source":["import numpy as np\n","import cv2 as cv\n","\n","img = cv.imread(\"Images/cityscape.jpg\", cv.IMREAD_GRAYSCALE)\n","\n","\n","\n","cv.line(img, (5,5), (200,150), (200,200,0), 10)\n","\n","cv.rectangle(img,(5,5), (200,150), (255,255,255), 8)\n","\n","cv.circle(img, (200,150), 50, (0,255,255), 20)\n","\n","points = np.array([[30,5], [300,200],[100,70],[40,100]], np.int32)\n","cv.polylines(img, [points], True, (255,0,0), 5)\n","\n","font = cv.FONT_HERSHEY_SIMPLEX\n","cv.putText(img, 'Salam be hame',(20,20), font, 1, (255,255,255), 2, cv.LINE_AA)\n","\n","cv.imshow('image',img)\n","cv.waitKey(0)\n","cv.destroyAllWindows()"],"metadata":{"id":"gsYydIJ0t0cu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.&nbsp;simple_operations"],"metadata":{"id":"Ax0GZkNkrUOF"}},{"cell_type":"code","source":["# Empty"],"metadata":{"id":"u2aj-48st5Ya"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4.&nbsp;logicalOperations"],"metadata":{"id":"v2-W1IRTrUMI"}},{"cell_type":"code","source":["import numpy as np\n","import cv2 as cv\n","\n","img1 = cv.imread('vsls1.jpg')\n","img2 = cv.imread('vsls2.jpg')\n","\n","#added = img1 + img2\n","\n","#added = cv.add(img1, img2)\n","#added = cv.addWeighted(img1, 0.2, img2, 0.8, 0)\n","\n","img2gray = cv.cvtColor(img2, cv.COLOR_BGR2GRAY)\n","ret, maskimg = cv.threshold(img2gray, 40 ,255, cv.THRESH_BINARY)\n","mask_inv = cv.bitwise_not(mask)\n","\n","img1_m = cv.bitwise_and(img1, img1, mask=maskimg)\n","\n","cv.imshow('image 1', img1)\n","cv.imshow('Mask', maskimg)\n","cv.imshow('img1_m', img1_m)\n","cv.waitKey(0)\n","cv.destroyAllWindows()"],"metadata":{"id":"51fDBbAXt52v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6.&nbsp;Morphological_Operations"],"metadata":{"id":"ge2fHHuLrUJt"}},{"cell_type":"code","source":["\n","\n","from cv2 import MORPH_OPEN\n","import numpy as np\n","import cv2 as cv\n","import matplotlib.pyplot as plt\n","\n","\n","original_img = cv.imread(\"images/cells_threshold.png\", 0)\n","\n","_, mask = cv.threshold(original_img, 25, 255, cv.THRESH_BINARY)\n","\n","# Erosion\n","kernel = np.ones((5,5), np.uint8)\n","#eroded_img = cv.erode(mask, kernel, iterations=1)\n","\n","# Dilation\n","kernel2 = np.ones((25,25), np.uint8)\n","delated_img = cv.dilate(mask, kernel, iterations=1)\n","#kernel = np.ones((3,3), np.uint8)\n","#eroded_img = cv.erode(delated_img, kernel, iterations=2)\n","subtract_img = delated_img - mask\n","\n","# Closing\n","#closed_img1 = cv.morphologyEx(mask, cv.MORPH_CLOSE, kernel1)\n","#closed_img2 = cv.morphologyEx(mask, cv.MORPH_CLOSE, kernel2)\n","\n","# Opening\n","#opened_img1 = cv.morphologyEx(mask, MORPH_OPEN, kernel1)\n","#opened_img2 = cv.morphologyEx(mask, MORPH_OPEN, kernel2)\n","\n","\n","# Gradient\n","\n","gradient = cv.morphologyEx(mask, cv.MORPH_GRADIENT, kernel)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","plt.subplot(1,2,1),plt.imshow(img,cmap = 'gray')\n","plt.title('Original'), plt.xticks([]), plt.yticks([])\n","plt.subplot(1,2,2),plt.imshow(laplacian,cmap = 'gray')\n","plt.title('Laplacian'), plt.xticks([]), plt.yticks([])\n","\n","plt.show()\n","\n","\"\"\"\n","plt.subplot(2,2,1),plt.imshow(img,cmap = 'gray')\n","plt.title('Original'), plt.xticks([]), plt.yticks([])\n","plt.subplot(2,2,2),plt.imshow(laplacian,cmap = 'gray')\n","plt.title('Laplacian'), plt.xticks([]), plt.yticks([])\n","plt.subplot(2,2,3),plt.imshow(sobelx,cmap = 'gray')\n","plt.title('Sobel X'), plt.xticks([]), plt.yticks([])\n","plt.subplot(2,2,4),plt.imshow(sobely,cmap = 'gray')\n","plt.title('Sobel Y'), plt.xticks([]), plt.yticks([])\n","\n","plt.show()\n","\"\"\""],"metadata":{"id":"xWOnf7NYt62V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 7.&nbsp;edge_Detection"],"metadata":{"id":"SBEULD-8rUHe"}},{"cell_type":"code","source":["\n","import numpy as np\n","import cv2 as cv\n","import matplotlib.pyplot as plt\n","\n","\n","image = cv.imread('images/Road_in_Norway.jpg', 0)\n","image_noise_removed = cv.GaussianBlur(image, (3,3),0)\n","\n","# Laplacian\n","laplacian = cv.Laplacian(image_noise_removed, cv.CV_64F)\n","\n","# sobelx\n","sobelx= cv.Sobel(image_noise_removed, cv.CV_64F, 1, 0, ksize=5)\n","\n","# sobely\n","sobely= cv.Sobel(image_noise_removed, cv.CV_64F, 0, 1, ksize=5)\n","\n","\n","# Canny Edge Detection\n","canny = cv.Canny(image_noise_removed, 100, 200)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\"\"\"\n","plt.subplot(1,2,1),plt.imshow(image,cmap = 'gray')\n","plt.title('Original'), plt.xticks([]), plt.yticks([])\n","plt.subplot(1,2,2),plt.imshow(canny,cmap = 'gray')\n","plt.title('Canny'), plt.xticks([]), plt.yticks([])\n","\n","plt.show()\n","\n","\"\"\"\n","plt.subplot(2,2,1),plt.imshow(image,cmap = 'gray')\n","plt.title('Original'), plt.xticks([]), plt.yticks([])\n","plt.subplot(2,2,2),plt.imshow(canny,cmap = 'gray')\n","plt.title('Canny'), plt.xticks([]), plt.yticks([])\n","plt.subplot(2,2,3),plt.imshow(sobelx,cmap = 'gray')\n","plt.title('Sobel X'), plt.xticks([]), plt.yticks([])\n","plt.subplot(2,2,4),plt.imshow(sobely,cmap = 'gray')\n","plt.title('Sobel Y'), plt.xticks([]), plt.yticks([])\n","\n","plt.show()\n"],"metadata":{"id":"RtTYX9Hmt-kI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 8.&nbsp;CornerDetection"],"metadata":{"id":"0A6uKGOVrUE4"}},{"cell_type":"code","source":["import numpy as np\n","import cv2 as cv\n","import matplotlib.pyplot as plt\n","\n","\n","# Read in the image\n","image = cv.imread('Images/shapes.png')\n","image_gr = cv.imread('Images/shapes.png', 0)\n","\n","corners = cv.cornerHarris(image_gr, 3, 10, 0.04)\n","corners_dilated = cv.dilate(corners, None)\n","\n","image[corners_dilated > 0.01*corners_dilated.max()] = [255,0,0]\n","\n","\n","plt.imshow(image)\n","plt.imshow()"],"metadata":{"id":"Tg7XpiWOt_o2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 9.&nbsp;TemplateMatching"],"metadata":{"id":"zKU5zf4CrUCW"}},{"cell_type":"code","source":["import numpy as np\n","import cv2 as cv\n","import matplotlib.pyplot as plt\n","\n","image = cv.imread(\"Images/building1.jpg\", 0)\n","\n","template = cv.imread(\"Images/building1_temp.jpg\", 0)\n","w, h = template.shape\n","result = cv.matchTemplate(image, template, cv.TM_CCOEFF_NORMED)\n","\n","threshold = 0.7\n","\n","locations = np.where(result >= threshold)\n","\n","for point in zip(*locations[::-1]):\n","    cv.rectangle(image, point, (point[0]+ h, point[1]+w), (255,255,0), 2)\n","\n","\n","plt.imshow(image)\n","plt.show()"],"metadata":{"id":"ajk7iQYXuQPd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 10.&nbsp;HistogramEqu"],"metadata":{"id":"Fm69Z_ZvrUAb"}},{"cell_type":"code","source":["\n","import cv2 as cv\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","image = cv.imread(\"images/xray1.jpg\", 0)\n","img_hist= cv.calcHist([image], [0],None,[256], [0,256])\n","equalized_histogram = cv.equalizeHist(image)\n","img_equal_hist= cv.calcHist([equalized_histogram], [0],None,[256], [0,256])\n","\n","clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n","cl1 = clahe.apply(image)\n","cl_equal_hist= cv.calcHist([cl1], [0],None,[256], [0,256])\n","\n","\n","plt.subplot(231), plt.imshow(image, 'gray')\n","plt.subplot(234), plt.plot(img_hist)\n","plt.subplot(232), plt.imshow(equalized_histogram, 'gray')\n","plt.subplot(235), plt.plot(img_equal_hist)\n","plt.subplot(233), plt.imshow(cl1, 'gray')\n","plt.subplot(236), plt.plot(cl_equal_hist)\n","plt.show()"],"metadata":{"id":"N1xIioe0uScW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 11.&nbsp;featureMatching"],"metadata":{"id":"kOtIsY9jrT9n"}},{"cell_type":"code","source":["import cv2 as cv\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","\n","image1 = cv.imread(\"images/book1.png\")\n","image2 = cv.imread(\"images/book2.png\")\n","\n","#ORB\n","feat_orb = cv.ORB_create(nfeatures=1000)\n","\n","\n","orb_keypoints1, descriptors1 = feat_orb.detectAndCompute(image1, None)\n","orb_keypoints2, descriptors2 = feat_orb.detectAndCompute(image2, None)\n","\n","bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck= True)\n","\n","matches = bf.match(descriptors1, descriptors2)\n","\n","matches = sorted(matches, key= lambda x:x.distance)\n","\n","image_matches = cv.drawMatches(image1, orb_keypoints1, image2, orb_keypoints2, matches[:100], None, flags=2)\n","\n","plt.imshow(image_matches)\n","plt.show()\n"],"metadata":{"id":"wh3wX47RuUdg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 12.&nbsp;ColorSpaces"],"metadata":{"id":"D7r1qTOTrT7s"}},{"cell_type":"code","source":["import numpy as np\n","import cv2 as cv\n","import matplotlib.pyplot as plt\n","\n","\n","image_bgr = cv.imread(\"Images/blue_eyes.jpg\")    #BGR\n","image_gr = cv.cvtColor(image_bgr, cv.COLOR_BGR2GRAY)\n","\n","#blue_ch, green_ch, red_ch = cv.split(image)\n","image_rgb = cv.cvtColor(image_bgr, cv.COLOR_BGR2RGB)\n","# RGB\n","# BGR  opencv\n","# BGRA / RGBA 4 channel alpha opacity\n","\n","# HSV [20, 100, 100]\n","image_hsv = cv.cvtColor(image_bgr, cv.COLOR_BGR2HSV)\n","\n","\n","\"\"\"\n","cv.imshow('IMAGE display', image)\n","#cv.imshow('Blue display', blue_ch)\n","#cv.imshow('RED display', red_ch)\n","#cv.imshow('Green display', green_ch)\n","cv.waitKey(0)\n","cv.destroyAllWindows()\n","\"\"\"\n","\n","plt.imshow(image_hsv)\n","plt.show()"],"metadata":{"id":"BUpPVkx6uWDy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 13.&nbsp;videoCam"],"metadata":{"id":"Ty7cySNerT5U"}},{"cell_type":"code","source":["import cv2 as cv\n","import numpy as np\n","\n","cap = cv.VideoCapture(0)\n","\n","while(True):\n","    rec, frame = cap.read()\n","    frame_hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n","\n","    lower_red = np.array([100,50,50])\n","    upper_red = np.array([116,255,255])\n","\n","    mask_red = cv.inRange(frame_hsv, lower_red, upper_red)\n","    frame_masked = cv.bitwise_and(frame, frame, mask = mask_red)\n","\n","    cv.imshow('frame', frame)\n","    cv.imshow('mask_red', mask_red)\n","    cv.imshow('frame_masked', frame_masked)\n","    keyexit = cv.waitKey(5) & 0xFF\n","    if keyexit == 27:\n","        break\n","\n","cv.destroyAllWindows()\n","cap.release()"],"metadata":{"id":"f9w_N3sxuX7W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 14.&nbsp;faceDetection"],"metadata":{"id":"wxEPnUNzrT3A"}},{"cell_type":"code","source":["import cv2 as cv\n","import numpy as np\n","\n","\n","face_cascade = cv.CascadeClassifier('haarcascade_frontalface_default.xml')\n","eye_cascade = cv.CascadeClassifier('haarcascade_eye.xml')\n","smile_cascade = cv.CascadeClassifier('haarcascade_smile.xml')\n","\n","cap = cv.VideoCapture(0)\n","\n","while(True):\n","    rec, frame = cap.read()\n","    frame_gr = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n","    faces = face_cascade.detectMultiScale(frame_gr, 1.3, 5)\n","    for (x, y, w, h) in faces:\n","        cv.rectangle(frame, (x,y), (x+w, y+h), (255,0,255), 2)\n","        frame_gr_roi = frame_gr[y:y+h, x:x+w]\n","        frame_roi = frame[y:y+h, x:x+w]\n","        eyes = eye_cascade.detectMultiScale(frame_gr_roi)\n","\n","        for (ex, ey, ew, eh) in eyes:\n","            cv.rectangle(frame_roi, (ex, ey), (ex+ew, ey+eh), (0,255,0), 2)\n","\n","        smiles = smile_cascade.detectMultiScale(frame_gr_roi, 1.8, 20)\n","\n","        for (sx, sy, sw, sh) in smiles:\n","            cv.rectangle(frame_roi, (sx, sy), (sx+sw, sy+sh), (0,0,255), 2)\n","\n","    cv.imshow('frame', frame)\n","\n","    keyexit = cv.waitKey(5) & 0xFF\n","    if keyexit == 27:\n","        break\n","\n","cv.destroyAllWindows()\n","cap.release()"],"metadata":{"id":"zvlOVSOhuZg7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 15.&nbsp;HandGesture_1"],"metadata":{"id":"HymjA6gKrT08"}},{"cell_type":"code","source":["import cv2 as cv\n","import numpy as np\n","from cvzone.HandTrackingModule import HandDetector\n","\n","cap = cv.VideoCapture(1)\n","detector = HandDetector(detectionCon = 0.5, maxHands=2)\n","\n","while(True):\n","    rec, frame = cap.read()\n","    hand, frame = detector.findHands(frame)\n","    if hand:\n","        hand1 = hand[0]\n","        lmlist1 = hand1[\"lmList\"]\n","\n","        length, info, frame = detector.findDistance(lmlist1[4][:-1], lmlist1[8][:-1], frame)\n","\n","    cv.imshow('frame', frame)\n","    keyexit = cv.waitKey(5) & 0xFF\n","    if keyexit == 27:\n","        break\n","\n","cv.destroyAllWindows()\n","cap.release()"],"metadata":{"id":"xpjNTutuubDn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 16.&nbsp;faceDetector_1"],"metadata":{"id":"8fLNx0KZrTyZ"}},{"cell_type":"code","source":["import cv2 as cv\n","import numpy as np\n","from cvzone.FaceDetectionModule import FaceDetector\n","from cvzone.FaceMeshModule import FaceMeshDetector\n","\n","cap = cv.VideoCapture(1)\n","detector = FaceDetector()\n","meshdetector = FaceMeshDetector(maxFaces=1)\n","\n","while(True):\n","    rec, frame = cap.read()\n","    frame, bbox = detector.findFaces(frame)\n","    frame, faces = meshdetector.findFaceMesh(frame)\n","\n","    if bbox:\n","        center = bbox[0][\"center\"]\n","        #cv.circle(frame, center, 5, (255, 0, 255), cv.FILLED)\n","    cv.imshow('frame', frame)\n","    keyexit = cv.waitKey(5) & 0xFF\n","    if keyexit == 27:\n","        break\n","\n","cv.destroyAllWindows()\n","cap.release()"],"metadata":{"id":"67_mWCKZuczo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 17.&nbsp;QRdetector_1"],"metadata":{"id":"11OA7y4hrTwL"}},{"cell_type":"code","source":["import numpy as np\n","import cv2 as cv\n","import qrcode\n","\n","\n","### Generate QRcode\n","generated_qr_code = qrcode.make(\"Learn Python\")\n","generated_qr_code.save('Images/qrcode3.png')\n","generated_qr_code_cv = np.array(generated_qr_code)\n","\n","### Detect QR code\n","qrcode_img = cv.imread(\"Images/qrcode3.png\")\n","\n","detector = cv.QRCodeDetector()\n","value, box, _ = detector.detectAndDecode(qrcode_img)\n","cv.rectangle(qrcode_img, (box[0][0][0], box[0][0][1]), (box[0][2][0], box[0][2][1]), (255,255,0), 2)\n","\n","\n","cv.imshow(\"qrcode_img\", qrcode_img)\n","cv.waitKey()\n","cv.destroyAllWindows()\n","\n"],"metadata":{"id":"daYJKZWXue94"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 17.&nbsp;QRdetector_2"],"metadata":{"id":"osJIDc50rTtx"}},{"cell_type":"code","source":["import pyzbar.pyzbar as pyzbar\n","import numpy as np\n","import cv2 as cv\n","from matplotlib import pyplot as plt\n","\n","barcode_img = cv.imread(\"Images/qrcode1.png\")\n","\n","\n","def QR_decode(im) :\n","    # Find barcodes and QR codes\n","    decodedObjects = pyzbar.decode(im)\n","    # Print results\n","    for obj in decodedObjects:\n","        print('Type : ', obj.type)\n","        print('Data : ', obj.data,'/n')\n","    return decodedObjects\n","\n","# Display barcode and QR code location\n","def display(im, decodedObjects):\n","    # Loop over all decoded objects\n","    for decodedObject in decodedObjects:\n","        points = decodedObject.polygon\n","        # If the points do not form a quad, find convex hull\n","        if len(points) > 4 :\n","          hull = cv.convexHull(np.array([point for point in points], dtype=np.float32))\n","          hull = list(map(tuple, np.squeeze(hull)))\n","        else :\n","          hull = points\n","        # Number of points in the convex hull\n","        n = len(hull)\n","        # Draw the convext hull\n","        for j in range(0,n):\n","          cv.line(im, hull[j], hull[ (j+1) % n], (255,0,0), 3)\n","    return im\n","\n","\n","cap = cv.VideoCapture(1)\n","while(True):\n","    rec, frame = cap.read()\n","    frame_gr = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n","    decodedObjects = QR_decode(frame_gr)\n","    for decodedObject in decodedObjects:\n","        points = decodedObject.polygon\n","\n","        # If the points do not form a quad, find convex hull\n","        if len(points) > 4 :\n","          hull = cv.convexHull(np.array([point for point in points], dtype=np.float32))\n","          hull = list(map(tuple, np.squeeze(hull)))\n","        else :\n","          hull = points;\n","\n","        # Number of points in the convex hull\n","        n = len(hull)\n","        # Draw the convext hull\n","        for j in range(0,n):\n","          cv.line(frame, hull[j], hull[ (j+1) % n], (255,0,0), 3)\n","\n","        x = decodedObject.rect.left\n","        y = decodedObject.rect.top\n","\n","        print(x, y)\n","\n","        print('Type : ', decodedObject.type)\n","        print('Data : ', decodedObject.data,'/n')\n","\n","        barCode = str(decodedObject.data)\n","        cv.putText(frame, barCode, (x, y), cv.FONT_HERSHEY_SIMPLEX, 1, (0,255,255), 2, cv.LINE_AA)\n","    cv.imshow('frame', frame)\n","    keyexit = cv.waitKey(5) & 0xFF\n","    if keyexit == 27:\n","        break\n","\n","cv.destroyAllWindows()\n","cap.release()"],"metadata":{"id":"c3vtdis3uqxr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 17.&nbsp;barcode_and_QR_scanner"],"metadata":{"id":"_QidVZEWr08y"}},{"cell_type":"code","source":["\n","import numpy as np\n","import cv2 as cv\n","from pyzbar.pyzbar import decode\n","\n","barcode_img = cv.imread(\"Images/barcode1.jpg\", 0)\n","decoded_barcode = decode(barcode_img)\n","print(decoded_barcode)"],"metadata":{"id":"3kZiwI7kuny5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 18.&nbsp;MovingobjectDetector"],"metadata":{"id":"xoTPhFB7r1iL"}},{"cell_type":"code","source":["import numpy as np\n","import cv2 as cv\n","\n","def rescale_frame(frame, percent=20):\n","    width = int(frame.shape[1] * percent/ 100)\n","    height = int(frame.shape[0] * percent/ 100)\n","    dim = (width, height)\n","    return cv.resize(frame, dim, interpolation =cv.INTER_AREA)\n","\n","\n","cap = cv.VideoCapture(\"Videos/street_camera.mp4\")\n","fps = cap.get(cv.CAP_PROP_FPS)\n","print(fps)\n","while(True):\n","    rec, frame1 = cap.read()\n","    rec, frame2 = cap.read()\n","\n","    resized_frame1 = rescale_frame(frame1)\n","    resized_frame2 = rescale_frame(frame2)\n","\n","    frame_diff = cv.absdiff(resized_frame1, resized_frame2)\n","    frame_diff_gr = cv.cvtColor(frame_diff, cv.COLOR_BGR2GRAY)\n","    blurred_frame = cv.GaussianBlur(frame_diff_gr, (9,9), 1)\n","    _, mask = cv.threshold(blurred_frame, 10, 255, cv.THRESH_BINARY)\n","\n","    contours, _ = cv.findContours(mask, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n","\n","\n","    for contour in contours:\n","        if cv.contourArea(contour)>1000:\n","            (x, y, w, h) = cv.boundingRect(contour)\n","            cv.rectangle(resized_frame1, (x + w -100, y + h-50), (x + w, y + h), (0, 0, 255), 2)\n","\n","    cv.imshow('frame', resized_frame1)\n","    cv.imshow('frame_diff', frame_diff)\n","    cv.imshow('mask', mask)\n","    keyexit = cv.waitKey(5) & 0xFF\n","    if keyexit == 27:\n","        break\n","\n","cv.destroyAllWindows()\n","cap.release()"],"metadata":{"id":"3KvGwjJNutQz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 19.&nbsp;Eyetracker"],"metadata":{"id":"FdLxf-q5r27y"}},{"cell_type":"code","source":["import cv2 as cv\n","import numpy as np\n","from cvzone.FaceDetectionModule import FaceDetector\n","from cvzone.FaceMeshModule import FaceMeshDetector\n","\n","LEFT_EYE =[362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398]\n","\n","detector = FaceDetector()\n","meshdetector = FaceMeshDetector(maxFaces=1)\n","\n","face_img = cv.imread(\"Images/face.jpg\")\n","face_img2 = face_img.copy()\n","\n","face_img, bbox = detector.findFaces(face_img)\n","face_img, faces = meshdetector.findFaceMesh(face_img)\n","\n","\n","\n","if bbox:\n","    center = bbox[0][\"center\"]\n","    if faces:\n","        left_eye_points = np.array([[faces[0][p][0],faces[0][p][1]] for p in LEFT_EYE])\n","        #cv.fillPoly(face_img2, pts=[left_eye_points], color = 255)\n","        (ex,ey,ew,eh) = cv.boundingRect(left_eye_points)\n","        #cv.rectangle(face_img2, (ex,ey), (ex+ew,ey+eh), (255,255,255))\n","        eye_roi = face_img2[ey:ey+eh, ex:ex+ew]\n","        eye_roi_gr = cv.cvtColor(eye_roi, cv.COLOR_BGR2GRAY)\n","        _, iris = cv.threshold(eye_roi_gr, 40, 255, cv.THRESH_BINARY_INV)\n","        contours, _ = cv.findContours(iris, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n","        contours = sorted(contours, key=lambda x: cv.contourArea(x), reverse=True)\n","\n","        if contours:\n","            (ix,iy,iw,ih) = cv.boundingRect(contours[0])\n","            ix_cntr, iy_centr = ix+int(iw/2) + ex, iy+int(ih/2)+ey\n","            cv.circle(face_img2, (ix_cntr, iy_centr), 5, (0,0,255), -1)\n","\n","            ix_cntr_e, iy_centr_e = ix+int(iw/2), iy+int(ih/2)\n","            if ix_cntr_e > int(ew/2):\n","                print(\"right\")\n","            elif ix_cntr_e < int(ew/2):\n","                print(\"left\")\n","\n","\n","\n","\n","\n","\n","cv.imshow('eye_roi', face_img2)\n","cv.waitKey(0)\n","cv.destroyAllWindows()"],"metadata":{"id":"ualtf28IutAZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 19.&nbsp;Eyetracker_vid"],"metadata":{"id":"SS9RCqWIr234"}},{"cell_type":"code","source":["import cv2 as cv\n","import numpy as np\n","from cvzone.FaceDetectionModule import FaceDetector\n","from cvzone.FaceMeshModule import FaceMeshDetector\n","\n","LEFT_EYE =[362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398]\n","\n","\n","detector = FaceDetector()\n","meshdetector = FaceMeshDetector(maxFaces=1)\n","\n","\n","cap = cv.VideoCapture(\"Videos/eye_tracker.mp4\")\n","if (cap.isOpened()== False):\n","    print(\"Error opening video stream or file\")\n","\n","\n","while(cap.isOpened()):\n","    ret, frame = cap.read()\n","    ret, frame2 = cap.read()\n","    if ret == True:\n","        face_img, bbox = detector.findFaces(frame)\n","        face_img, faces = meshdetector.findFaceMesh(frame)\n","        print(bbox)\n","        if bbox:\n","            center = bbox[0][\"center\"]\n","            if faces:\n","                left_eye_points = np.array([[faces[0][p][0],faces[0][p][1]] for p in LEFT_EYE])\n","                #cv.fillPoly(face_img2, pts=[left_eye_points], color = 255)\n","                (ex,ey,ew,eh) = cv.boundingRect(left_eye_points)\n","                #cv.rectangle(face_img2, (ex,ey), (ex+ew,ey+eh), (255,255,255))\n","                eye_roi = frame2[ey:ey+eh, ex:ex+ew]\n","                eye_roi_gr = cv.cvtColor(eye_roi, cv.COLOR_BGR2GRAY)\n","                _, iris = cv.threshold(eye_roi_gr, 40, 255, cv.THRESH_BINARY_INV)\n","                contours, _ = cv.findContours(iris, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n","                contours = sorted(contours, key=lambda x: cv.contourArea(x), reverse=True)\n","\n","                if contours:\n","                    (ix,iy,iw,ih) = cv.boundingRect(contours[0])\n","                    ix_cntr, iy_centr = ix+int(iw/2) + ex, iy+int(ih/2)+ey\n","                    cv.circle(frame2, (ix_cntr, iy_centr), 5, (0,0,255), -1)\n","\n","                    ix_cntr_e, iy_centr_e = ix+int(iw/2), iy+int(ih/2)\n","\n","                    offset = 10\n","                    if ix_cntr_e > int(ew/2)+offset:\n","                        text = \"right\"\n","                    elif ix_cntr_e < int(ew/2)-offset:\n","                        text = \"left\"\n","                    else:\n","                        text = \"center\"\n","\n","                    cv.putText(frame2, text, (100,100), cv.FONT_HERSHEY_PLAIN, 3, (0,60,0), 2)\n","\n","\n","        cv.imshow('frame2', frame2)\n","        if cv.waitKey(25) & 0xFF == ord('q'):\n","            break\n","    else:\n","        break\n","\n","cap.release()\n","cv.destroyAllWindows()"],"metadata":{"id":"W9QPYM3pusyw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 00.&nbsp;DL_4_2_Object_Detection_OpenCV_yolov3"],"metadata":{"id":"TFugr1LRr2z6"}},{"cell_type":"code","source":["import os\n","import cv2 as cv\n","import numpy as np\n","\n","yolo_path = \"/data/YOLO\"\n","\n","classes = []\n","with open(\"/data/YOLO/coco.names\", \"r\") as f:\n","    classes = [line.strip() for line in f.readlines()]\n","\n","yolo_net = cv.dnn.readNet(os.path.join(yolo_path,\"yolov3.weights\"), os.path.join(yolo_path,\"yolov3.cfg\"))\n","layer_names = yolo_net.getLayerNames()\n","output_layers = [layer_names[i[0] - 1] for i in yolo_net.getUnconnectedOutLayers()]\n","colors = np.random.uniform(0, 255, size=(len(classes), 3))\n","colors[0],colors[1],colors[2],colors[5],colors[9] = (176, 104, 176), (226, 43, 138), (255, 191, 0), (153, 255, 255), (0, 136, 255)\n","cap = cv.VideoCapture(\"/Videos/driving_camera.mp4\")\n","video_cod = cv.VideoWriter_fourcc(*'XVID')\n","video_output = cv.VideoWriter('driving_camera_det_yolov3_cv.avi',\n","                      video_cod,\n","                      10,\n","                      (1280,720))\n","while(cap.isOpened()):\n","    ret, img = cap.read()\n","    if ret == True:\n","        #img = cv.resize(frame, None, fx=0.8, fy=0.8)\n","        height, width, channels = img.shape\n","        #print(height, width)\n","        blob = cv.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n","        yolo_net.setInput(blob)\n","        outs = yolo_net.forward(output_layers)\n","        class_ids = []\n","        confidences = []\n","        boxes = []\n","        for out in outs:\n","            for detection in out:\n","                scores = detection[5:]\n","                class_id = np.argmax(scores)\n","                confidence = scores[class_id]\n","                if confidence > 0.5:\n","                    # Object detected\n","                    center_x = int(detection[0] * width)\n","                    center_y = int(detection[1] * height)\n","                    w = int(detection[2] * width)\n","                    h = int(detection[3] * height)\n","\n","                    # Rectangle coordinates\n","                    x = int(center_x - w / 2)\n","                    y = int(center_y - h / 2)\n","\n","                    boxes.append([x, y, w, h])\n","                    confidences.append(float(confidence))\n","                    class_ids.append(class_id)\n","\n","        indexes = cv.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n","        font = cv.FONT_HERSHEY_PLAIN\n","        for i in range(len(boxes)):\n","            if i in indexes:\n","                x, y, w, h = boxes[i]\n","                label = str(classes[class_ids[i]])\n","                color = colors[class_ids[i]]\n","                cv.rectangle(img, (x, y), (x + w, y + h), color, 2)\n","                cv.putText(img, label, (x, y + 30), font, 1, color, 1)\n","        cv.imshow('Frame',img)\n","        video_output.write(img)\n","\n","        if cv.waitKey(25) & 0xFF == ord('q'):\n","            break\n","    else:\n","        break\n","\n","cap.release()\n","video_output.release()\n","cv.destroyAllWindows()"],"metadata":{"id":"KHjIGEjBu68l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 00.&nbsp;DL_4_3_Object_Detection_OpenCV_yolov3"],"metadata":{"id":"RIhwemyHr2wD"}},{"cell_type":"code","source":["import os\n","import cv2 as cv\n","import numpy as np\n","\n","yolo_path = \"/data/YOLO\"\n","\n","classes = []\n","with open(\"/data/YOLO/coco.names\", \"r\") as f:\n","    classes = [line.strip() for line in f.readlines()]\n","\n","yolo_net = cv.dnn.readNet(os.path.join(yolo_path,\"yolov3.weights\"), os.path.join(yolo_path,\"yolov3.cfg\"))\n","layer_names = yolo_net.getLayerNames()\n","output_layers = [layer_names[i[0] - 1] for i in yolo_net.getUnconnectedOutLayers()]\n","colors = np.random.uniform(0, 255, size=(len(classes), 3))\n","cap = cv.VideoCapture(0)\n","\n","while(cap.isOpened()):\n","    ret, img = cap.read()\n","    if ret == True:\n","        #img = cv.resize(frame, None, fx=0.8, fy=0.8)\n","        height, width, channels = img.shape\n","        print(height, width)\n","        blob = cv.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n","        yolo_net.setInput(blob)\n","        outs = yolo_net.forward(output_layers)\n","        class_ids = []\n","        confidences = []\n","        boxes = []\n","        for out in outs:\n","            for detection in out:\n","                scores = detection[5:]\n","                class_id = np.argmax(scores)\n","                confidence = scores[class_id]\n","                if confidence > 0.5:\n","                    # Object detected\n","                    center_x = int(detection[0] * width)\n","                    center_y = int(detection[1] * height)\n","                    w = int(detection[2] * width)\n","                    h = int(detection[3] * height)\n","\n","                    # Rectangle coordinates\n","                    x = int(center_x - w / 2)\n","                    y = int(center_y - h / 2)\n","\n","                    boxes.append([x, y, w, h])\n","                    confidences.append(float(confidence))\n","                    class_ids.append(class_id)\n","\n","        indexes = cv.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n","        print(indexes)\n","        font = cv.FONT_HERSHEY_PLAIN\n","        for i in range(len(boxes)):\n","            if i in indexes:\n","                x, y, w, h = boxes[i]\n","                label = str(classes[class_ids[i]])\n","                color = colors[class_ids[i]]\n","                cv.rectangle(img, (x, y), (x + w, y + h), color, 2)\n","                cv.putText(img, label, (x, y + 30), font, 2, color, 2)\n","        cv.imshow('Frame',img)\n","        if cv.waitKey(25) & 0xFF == ord('q'):\n","            break\n","    else:\n","        break\n","\n","cap.release()\n","cv.destroyAllWindows()"],"metadata":{"id":"bunGmaauu6wp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 00.&nbsp;DL_5_Eyetracker"],"metadata":{"id":"IvoLqja8r2rm"}},{"cell_type":"code","source":["import cv2 as cv\n","import numpy as np\n","from cvzone.FaceDetectionModule import FaceDetector\n","from cvzone.FaceMeshModule import FaceMeshDetector\n","\n","cap = cv.VideoCapture(0)\n","detector = FaceDetector()\n","meshdetector = FaceMeshDetector(maxFaces=1)\n","\n","while(True):\n","    rec, frame = cap.read()\n","    rec, frame2 = cap.read()\n","    frame, bbox = detector.findFaces(frame)\n","    frame, faces = meshdetector.findFaceMesh(frame)\n","\n","    if bbox:\n","        center = bbox[0][\"center\"]\n","        i=15\n","        if faces:\n","            for i in range(100, len(faces[0])):\n","                cv.putText(frame2, str(i), (faces[0][i][0],faces[0][i][1]), cv.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv.LINE_AA)\n","\n","        #cv.circle(frame, center, 5, (255, 0, 255), cv.FILLED)\n","    cv.imshow('frame', frame2)\n","    keyexit = cv.waitKey(5) & 0xFF\n","    if keyexit == 27:\n","        break\n","\n","cv.destroyAllWindows()\n","cap.release()"],"metadata":{"id":"8kleaU1Eu6jO"},"execution_count":null,"outputs":[]}]}